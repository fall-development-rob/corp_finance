# ADR-002: RuVector Integration Hardening and Optimization

| Field       | Value                        |
|-------------|------------------------------|
| **Status**  | Proposed                     |
| **Date**    | 2026-02-13                   |
| **Authors** | Robert Fall                  |
| **Deciders**| Engineering / Architecture   |

---

## Context

The CFA agent system (ADR-001) uses **ruvector-postgres** v2.0.0 as its vector database for
embedding storage and HNSW-based similarity search. RuVector is a Rust-native PostgreSQL
extension that provides 290+ SQL functions, SIMD-accelerated distance calculations (AVX2/AVX-512/NEON),
multi-tenancy, and advanced features including attention mechanisms, GNN layers, and hybrid search.

The current integration works but has several issues discovered during implementation and testing:

- **HNSW segfault** (ruvector Issue #164): The extension crashes with SIGSEGV on the first
  vector similarity search after inserting embeddings, forcing Postgres into recovery mode.
  We have a `queryWithRetry()` workaround that adds 3-6 seconds of latency on first search.
- **Dead schema**: 6 of 9 tables in the migration (`pattern_embeddings`, `pattern_links`,
  `agent_sessions`, `agent_memories`, `agent_tasks`, `agent_events`) are defined but never
  queried by any TypeScript code. Their HNSW indexes consume memory and slow inserts.
- **Silent embedding degradation**: If the `@xenova/transformers` model fails to load,
  `computeEmbedding` falls back to hash-based pseudo-random vectors without warning.
  All vector searches then return random results.
- **Missing runtime tuning**: `ruvector.ef_search` (query-time recall control) is never set,
  defaulting to 40. No `SET` statement is issued at connection time.
- **No pool error handling**: The `pg.Pool` singleton has no `on('error')` handler. An idle
  connection error from a ruvector segfault crashes the Node process.
- **Suboptimal ticker lookups**: `getByTicker()` embeds a bare ticker symbol and does HNSW
  cosine search, when a `WHERE 'AAPL' = ANY(tags)` query against the existing GIN index
  would be faster and more accurate.
- **Vector literal bloat**: `float32ToVectorLiteral()` produces full-precision decimal strings
  (~3.5KB per 384-dim vector). Fixed 6-decimal precision would cut network I/O by ~30%.
- **In-memory stats**: `PgReasoningBank` counters reset on process restart. Should be derived
  from database aggregates.
- **No deduplication**: Duplicate tool sequence patterns create new rows every time. The
  fingerprint field exists but is not used for upsert.
- **No transaction wrapping**: Migrations run as individual `pool.query()` calls. A
  multi-statement migration that fails midway leaves the database partially migrated.
- **Docker container untuned**: No `shm_size`, `shared_buffers`, `maintenance_work_mem`, or
  resource limits configured for vector workloads.

### Current Usage Surface

The integration uses a narrow subset of ruvector's capabilities:

| Used | Not Used |
|------|----------|
| `ruvector(384)` type | `halfvec(384)` (50% memory savings) |
| `ruvector_cosine_ops` HNSW | `ruvector_ip_ops` (faster for normalized vectors) |
| `<=>` cosine distance | `<#>` inner product (avoids `1 - x` computation) |
| `search_reasoning_memories` function | `min_similarity` parameter (always 0.0) |
| Ad-hoc text queries | Prepared statements |
| Individual INSERTs | `COPY` / batch `UNNEST` |

### ruvector Known Issues

| Issue | Severity | Impact on CFA Agents |
|-------|----------|---------------------|
| #164: HNSW segfault on large tables | Critical | Triggers on small tables too; mitigated by `queryWithRetry` |
| #152: HNSW breaks COUNT(*) queries | Medium | Not currently affected (no COUNT on embedding tables) |
| docs.rs build failure for v2.0.1 | Low | Documentation unavailable; relying on GitHub README |
| lru 0.12.5 Stacked Borrows violation | Low | Security advisory; no direct exploit path |

---

## Decision

### 1. Pool Resilience Layer

Add defensive pool management to `pg-client.ts`:

```
Pool Creation
  ├── on('error') handler → resetPool() + log warning (never crash)
  ├── on('connect') → SET ruvector.ef_search = 100
  ├── statement_timeout = 30000 (30s guard against hung HNSW queries)
  └── application_name = 'cfa-agents' (for pg_stat_activity visibility)

queryWithRetry()
  ├── Exponential backoff: 1s → 3s → 9s (instead of fixed 3s)
  ├── Max retries configurable via PG_RETRY_MAX env var
  └── Structured error logging with attempt count and query context
```

- Register `pool.on('error', ...)` to catch idle connection kills without crashing the process.
- Issue `SET ruvector.ef_search = 100` on every new connection via the pool's `connect` event
  to improve recall from 85% (default 40) to 90%.
- Set `statement_timeout = 30000` to prevent runaway HNSW queries from blocking the pool.
- Switch `queryWithRetry` to exponential backoff and make retry params env-configurable.

### 2. Embedding Quality Guard

Add a validation layer to `computeEmbedding` results before storing or searching:

```
computeEmbedding(text)
  ├── Check: variance > 0.001 (hash fallback produces low-variance vectors)
  ├── Check: L2 norm within [0.9, 1.1] (model output is normalized)
  └── On failure: throw EmbeddingQualityError (never silently degrade)
```

- Compute variance of the returned `Float32Array`. Hash-based fallback vectors have
  characteristic low variance from the `sin/cos` generation pattern.
- Verify the L2 norm is approximately 1.0 (all-MiniLM-L6-v2 produces unit vectors).
- If either check fails, throw an explicit error rather than storing meaningless embeddings
  that pollute future search results.

### 3. Schema Optimization (Migration 002)

Create a new migration `002_schema_optimization.sql`:

**a. Drop unused HNSW indexes** — saves memory and eliminates insert overhead:
```sql
DROP INDEX IF EXISTS idx_pe_embedding_hnsw;
DROP INDEX IF EXISTS idx_am_embedding_hnsw;
```

**b. Add partial index for pattern lookups**:
```sql
CREATE INDEX IF NOT EXISTS idx_rm_patterns
  ON reasoning_memories (domain, created_at DESC)
  WHERE type = 'reasoning_memory';
```

**c. Add `updated_at` trigger**:
```sql
CREATE OR REPLACE FUNCTION update_updated_at()
RETURNS TRIGGER AS $$
BEGIN NEW.updated_at = now(); RETURN NEW; END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_rm_updated_at
  BEFORE UPDATE ON reasoning_memories
  FOR EACH ROW EXECUTE FUNCTION update_updated_at();
```

**d. Add fingerprint unique constraint for pattern deduplication**:
```sql
ALTER TABLE reasoning_memories
  ADD COLUMN IF NOT EXISTS fingerprint TEXT;

CREATE UNIQUE INDEX IF NOT EXISTS idx_rm_fingerprint
  ON reasoning_memories (fingerprint)
  WHERE fingerprint IS NOT NULL;
```

Leave the unused tables in place (no data loss risk) but do not create new indexes on them.

### 4. Query Path Optimization

**a. Use GIN index for ticker lookups** in `PgFinancialMemory.getByTicker()`:
```sql
SELECT id, content, domain, tags, confidence, usage_count
FROM reasoning_memories
WHERE $1 = ANY(tags) AND domain = $2
ORDER BY confidence DESC, created_at DESC
LIMIT $3
```
This leverages `idx_rm_tags` (GIN) instead of computing an embedding and doing HNSW search.

**b. Use inner product instead of cosine** for normalized vectors:
Change `ruvector_cosine_ops` to `ruvector_ip_ops` in the HNSW index. Since all-MiniLM-L6-v2
produces unit vectors, inner product and cosine similarity are equivalent, but inner product
avoids the normalization step in the distance calculation.

**c. Pass `min_similarity` threshold** to `search_reasoning_memories`:
```sql
SELECT * FROM search_reasoning_memories($1::ruvector, $2, $3, 0.3)
```
Filter out low-quality matches (similarity < 0.3) at the database level instead of
returning all results and relying on the caller to interpret quality.

**d. Reduce vector literal size** in `float32ToVectorLiteral`:
```typescript
parts.push(vec[i].toFixed(6));  // ~2.5KB vs ~3.5KB per 384-dim vector
```

### 5. Pattern Deduplication with Upsert

Replace `INSERT` in `PgReasoningBank.recordTrace()` with:
```sql
INSERT INTO reasoning_memories (id, ..., fingerprint, confidence, usage_count)
VALUES ($1, ..., $fingerprint, 0.5, 1)
ON CONFLICT (fingerprint) DO UPDATE SET
  confidence = (reasoning_memories.confidence * reasoning_memories.usage_count + 0.5)
               / (reasoning_memories.usage_count + 1),
  usage_count = reasoning_memories.usage_count + 1,
  last_used_at = now()
```

This merges duplicate tool sequences into a single pattern with a running average reward
score and incremented usage count, instead of creating unbounded duplicate rows.

### 6. Persistent Stats from Database

Replace in-memory counters in `PgReasoningBank` with lazy-loaded database queries:

```typescript
async getStats() {
  const { rows } = await queryWithRetry<{ patterns: number; traces: number; avg_reward: number }>(
    `SELECT
       (SELECT count(*) FROM reasoning_memories WHERE type = 'reasoning_memory' AND fingerprint IS NOT NULL) AS patterns,
       (SELECT count(*) FROM task_trajectories) AS traces,
       (SELECT coalesce(avg(confidence), 0) FROM reasoning_memories WHERE fingerprint IS NOT NULL) AS avg_reward`,
    [],
  );
  return { totalPatterns: rows[0].patterns, totalTraces: rows[0].traces, avgReward: rows[0].avg_reward };
}
```

### 7. Docker Container Tuning

Update `docker-compose.yml`:

```yaml
services:
  ruvector-postgres:
    image: ruvnet/ruvector-postgres:2.0.0  # pin version
    shm_size: '256mb'
    command: >
      postgres
      -c shared_buffers=128MB
      -c effective_cache_size=256MB
      -c maintenance_work_mem=64MB
      -c work_mem=16MB
      -c max_connections=50
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '2.0'
```

- Pin the image version for reproducible builds.
- Set `shm_size` to 256MB for shared buffer backing.
- Tune `shared_buffers`, `maintenance_work_mem` (HNSW build speed), and `work_mem`.
- Set resource limits to prevent OOM from runaway HNSW operations.

### 8. Migration Transaction Safety

Wrap each migration in an explicit transaction in `runMigrations()`:

```typescript
const client = await pool.connect();
try {
  await client.query('BEGIN');
  await client.query(sql);
  await client.query(`INSERT INTO schema_migrations (version) VALUES ($1)`, [version]);
  await client.query('COMMIT');
} catch (err) {
  await client.query('ROLLBACK');
  throw err;
} finally {
  client.release();
}
```

Record the version inside the transaction so it only persists if the migration succeeds.

---

## Consequences

### Positive

- **Segfault resilience**: Pool error handlers and exponential backoff prevent process crashes
  and reduce retry latency from a fixed 6s to ~1-4s average.
- **Search quality**: `ef_search = 100` improves recall from ~85% to ~90%. `min_similarity`
  threshold eliminates irrelevant results at the database level.
- **Embedding integrity**: Quality guard prevents silent degradation to hash-based vectors,
  surfacing configuration errors immediately instead of producing incorrect search results.
- **Memory savings**: Dropping 2 unused HNSW indexes frees ~3KB per vector per index.
  Using `toFixed(6)` reduces network I/O by ~30% per embedding operation.
- **Pattern quality**: Fingerprint-based upsert prevents unbounded duplicate rows and
  maintains running average reward scores that reflect actual pattern utility.
- **Ticker lookup speed**: GIN-based ticker queries are O(1) lookups vs O(log n) HNSW
  traversal, and return exact matches instead of approximate nearest neighbors.
- **Operational visibility**: `application_name`, `statement_timeout`, and structured logging
  make database issues diagnosable from `pg_stat_activity` and application logs.
- **Migration safety**: Transaction wrapping prevents partial schema corruption.

### Negative

- **Index rebuild required**: Changing from `ruvector_cosine_ops` to `ruvector_ip_ops` requires
  dropping and recreating the HNSW index, which takes O(n log n) time. At current scale
  (< 1000 rows) this is negligible; at 100K+ rows it could take minutes.
- **Embedding validation overhead**: Variance and norm checks add ~0.1ms per embedding
  operation. Negligible vs the ~50ms embedding generation time.
- **Breaking change for `getByTicker`**: Switching from vector search to GIN lookup changes
  the result semantics (exact tag match vs approximate similarity). Callers that relied on
  fuzzy matching by ticker name will see different results.

### Neutral

- **Unused tables remain**: We drop indexes but keep the table definitions to avoid data loss
  and preserve forward compatibility if future features need them.
- **ruvector segfault root cause unresolved**: The retry logic mitigates but does not fix the
  underlying extension bug. This remains a risk if ruvector is not patched.
- **No halfvec migration yet**: Converting `ruvector(384)` columns to `halfvec(384)` would
  save 50% storage but requires data migration and reindexing. Deferred until table size
  exceeds 100K rows or memory pressure is observed.

---

## Alternatives Considered

### A. Replace ruvector with pgvector

Use the standard `pgvector` extension (v0.7+) instead of ruvector.

**Rejected because**: pgvector lacks the advanced features we plan to leverage (multi-tenancy,
hybrid search, attention mechanisms, self-healing indexes). The segfault issue is specific to
HNSW index state after inserts and is mitigated by `queryWithRetry`. RuVector's SIMD
acceleration (3.7x AVX2 speedup) and 290+ SQL functions provide a growth path that pgvector
cannot match. If ruvector stability does not improve by the time the dataset exceeds 100K
rows, we will revisit this decision.

### B. Move vector search to application layer (hnswlib-node)

Use `hnswlib-node` for in-process HNSW search, eliminating Postgres vector operations entirely.

**Rejected because**: This loses durability, multi-process access, and SQL-based filtering.
The pipeline runs across multiple invocations and potentially multiple processes; a shared
database is the correct persistence layer. Application-layer HNSW also duplicates the index
in memory alongside the database, increasing resource usage.

### C. Use Supabase/gte-small for embeddings instead of local all-MiniLM-L6-v2

Use an API-based embedding service for higher-quality vectors.

**Rejected because**: The pipeline must work offline and without API rate limits. Local
embeddings (384-dim, ~50ms per call) are fast enough for the current workload. The
`createRouterEmbedder` adapter already abstracts the embedding source, so switching to an
API-based service is a configuration change if higher-dimensional embeddings are needed later.

### D. Drop all unused tables in migration 002

Remove the 6 unused tables entirely to simplify the schema.

**Rejected because**: The tables contain no data and consume negligible space (empty tables
with no rows have zero storage cost). Dropping them removes forward compatibility for
features described in ADR-001 (agent sessions, per-agent memory, event sourcing). We drop
only the unused HNSW indexes, which have a real cost (memory + insert overhead).
